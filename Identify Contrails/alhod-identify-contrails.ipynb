{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello!\n\nWelcome to my \"Google Research - Identify Contrails to Reduce Global Warming\" submission. Here's the rundown:\n* UNET for image segmentation\n* A lot of stuff I copied from other people sry","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np\nimport os\nimport pandas as pd\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.109901Z","iopub.execute_input":"2023-06-29T16:26:16.110402Z","iopub.status.idle":"2023-06-29T16:26:16.118854Z","shell.execute_reply.started":"2023-06-29T16:26:16.110357Z","shell.execute_reply":"2023-06-29T16:26:16.116877Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class ContrailsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, train=True):\n        \n        # Pass in dataframe (3 columns: record_id, train, path)\n        # Note: ContrailsDataset object doesn't store images. Only stores path \n        # to images (from 'df'). Each time ContrailsDataset object is indexed \n        # into, it retrieves the sample and label corresponding to the index. \n        self.df = df\n        self.trn = train\n    \n    # Handles indexing into ContrailsDataset object (i.e. '[]' operation)\n    def __getitem__(self, index):\n        \n        # Accesses sample and label here (according to index)\n        row = self.df.iloc[index]\n        con_path = row.path\n        con = np.load(str(con_path))\n        \n        # Selects all dimensions before last one. In last dimension, selects all\n        # elements, excluding last one (':-1' means slice to but not include last \n        # one).\n        # All dimensions excluding last element of last one makes up the sample image.\n        img = con[..., :-1]\n        \n        # Selects all dimensions before last one + last element of last dimension\n        # All dimensions + last element of last one make up label\n        label = con[..., -1]\n        \n        img = torch.tensor(img)\n        label = torch.tensor(label)\n        \n        img = img.permute(2, 0, 1)\n        \n        display(\"indexing into thing\")\n#         display(img.shape)\n            \n        # Returns tuple !!! (sample, label)\n        # Indexing into ContrailDataset returns both sample and label!\n        return img.float(), label.float()\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.121294Z","iopub.execute_input":"2023-06-29T16:26:16.122884Z","iopub.status.idle":"2023-06-29T16:26:16.135889Z","shell.execute_reply.started":"2023-06-29T16:26:16.122830Z","shell.execute_reply":"2023-06-29T16:26:16.134505Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            \n            # in channels, out channels, kernel size, stride, padding\n            # padding = 1 --> input height and width is same before and after convolution\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n    \n\nclass UNET(nn.Module):\n    def __init__(\n        \n        # features are the numbers at the top of the blocks\n        self, in_channels=3, out_channels=1, features=[64, 128, 256, 512] \n    ):\n        super(UNET, self).__init__();\n        \n        # store convolution layers, since we want to do batch eval\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        # 161 x 161 --> 80 x 80 --> output: 160x160\n        # Input must be divisible by 16, since it is being divided by 2 four times (however we generalized it later on)\n        \n        \n        # Down part of UNET (where \"U\" goes down)\n        for feature in features:\n            \n            # Append layer into down\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n            \n        # Up part of UNET (where \"U\" goes up)\n        # We used transpose convolutions\n        for feature in reversed(features):\n            self.ups.append(\n                nn.Conv2d(\n                    feature*2, feature, kernel_size=2, stride=2\n                )\n            )\n            \n            # Append double conv (where U transition from down to up)\n            self.ups.append(DoubleConv(feature*2, feature))\n            \n            self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n            self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n            \n    def forward(self, x):\n        \n#         display('x:')\n#         display(x.shape)\n#         display(x)\n        \n        # Store skipped connections here\n        # Note that we are adding back the skipped connections in the reverse order we are adding into this list\n        skip_connections = []\n        \n        # Iterate through each convolutional layer in downs\n        for down in self.downs:\n            \n            # Apply convolutional layer on x\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n            \n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n        \n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            \n            # Generalizes when the width/height of layer isn't divisible by 2\n            if x.shape != skip_connection.shape:\n                \n                # \"skip_connection.shape[2:]\" gets the height and width (skipping batch size and channels)\n                x = TF.resize(x, size=skip_connection.shape[2:])\n                \n            \n            # Concatenate skip connections\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n        \n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.138256Z","iopub.execute_input":"2023-06-29T16:26:16.138997Z","iopub.status.idle":"2023-06-29T16:26:16.156977Z","shell.execute_reply.started":"2023-06-29T16:26:16.138956Z","shell.execute_reply":"2023-06-29T16:26:16.155677Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Utility Functions","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.158337Z","iopub.execute_input":"2023-06-29T16:26:16.158753Z","iopub.status.idle":"2023-06-29T16:26:16.171992Z","shell.execute_reply.started":"2023-06-29T16:26:16.158721Z","shell.execute_reply":"2023-06-29T16:26:16.171049Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.174572Z","iopub.execute_input":"2023-06-29T16:26:16.175434Z","iopub.status.idle":"2023-06-29T16:26:16.184786Z","shell.execute_reply.started":"2023-06-29T16:26:16.175389Z","shell.execute_reply":"2023-06-29T16:26:16.183692Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def get_loaders(\n    train_df,\n    valid_df,\n    batch_size,\n    train_transform,\n    val_transform,\n    num_workers=4,\n    pin_memory=True\n):\n    \n    # Get Contrails Dataset (class: ContrailsDataset, inherited: torch.data.Dataset)\n    train_ds = ContrailsDataset(\n        train_df,\n        train=True\n    )\n\n    # Wraps Dataset using DataLoader, which makes it iterable\n    #   'train_ds' is the pytorch.Dataset object\n    #   'batch_size' is the size of the \"mini-batches\" we want to load the data in\n    #   'pin_memory' \"For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.\"\n    #   'shuffle' will shuffle the data before every epoch\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True\n    )\n\n    valid_ds = ContrailsDataset(\n        valid_df,\n        train=False\n    )\n\n    valid_loader = DataLoader(\n        valid_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False\n    )\n\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.213085Z","iopub.execute_input":"2023-06-29T16:26:16.213514Z","iopub.status.idle":"2023-06-29T16:26:16.221484Z","shell.execute_reply.started":"2023-06-29T16:26:16.213480Z","shell.execute_reply":"2023-06-29T16:26:16.220123Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            print(f\"x.shape {x.shape}, y.shape {y.shape}\")\n            \n            x = x.to(device)\n            \n            # Label doesn't have channel because grayscale, so must .unsqueeze(1)\n            y = y.to(device).unsqueeze(1)\n\n            # This is for binary\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n\n            # Dice score (at least in this case) is a better metric for evaluating accuracy of segmentation (as if model outputs all black pixels it can get 80% accuracy)\n            dice_score += (2*(preds*y).sum())/(\n                (preds+y).sum() + 1e-8\n            )\n\n    # Display accuracy\n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n    )\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    \n    model.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.223862Z","iopub.execute_input":"2023-06-29T16:26:16.224254Z","iopub.status.idle":"2023-06-29T16:26:16.238737Z","shell.execute_reply.started":"2023-06-29T16:26:16.224224Z","shell.execute_reply":"2023-06-29T16:26:16.237713Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def save_predictions_as_imgs(\n    loader, model, folder=\"saved_images/\", device=\"cuda\"\n):\n    model.eval()\n    \n    # Iterates over each batch, NOT (unless BATCH_SIZE=1,) EACH SAMPLE\n    for idx, (x, y) in enumerate(loader):\n        \n        # Moves \"x\" to the specified device (usually for sake of efficiency, e.g. when training a model)\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            \n        # torchvision.utils.save_image (https://pytorch.org/vision/stable/generated/torchvision.utils.save_image.html):\n        #   Arguments:\n        #       Tensor\n        #           The Tensor to be saved as an image.\n        #       String/File\n        #           Location to save image\n        #   \"If given a mini-batch tensor, saves the tensor as a grid of images by calling make_grid.\"\n        #       I.e. this situation\n        \n        # torchvision.utils.make_grid (https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html):\n        #   Arguments:\n        #       Tensor\n        #           \"4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size.\"\n        #   Returns:\n        #       Tensor\n        #           Contains grid of images\n        #   Note: One of the arguments is \"nrow,\" which specifies the number of images in each row. The default is 8, \n        #       thus the predictions have 8 images per row.\n         \n        #   Note: We pass in a 4D tensor (preds), of shape [B, C, H, W] (\"mini batch\" size, channels, height, width).\n        #       Thus, torchvision.utils.make_grid is called and we get a total of 16 images per grid, with 8 images \n        #       per row. If we want the single image predictions, we can set BATCH_SIZE = 1, or just have no batches\n        #       at all.\n        torchvision.utils.save_image(\n            preds, f\"{folder}/pred_{idx}.png\"\n        )\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/{idx}.png\")\n        \n    model.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.240435Z","iopub.execute_input":"2023-06-29T16:26:16.240804Z","iopub.status.idle":"2023-06-29T16:26:16.257126Z","shell.execute_reply.started":"2023-06-29T16:26:16.240774Z","shell.execute_reply":"2023-06-29T16:26:16.255684Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"### Hyperparameters and more","metadata":{}},{"cell_type":"code","source":"# Hyperparameters etc.\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 1\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 160\nIMAGE_WIDTH = 240\nPIN_MEMORY = True\n\n# Set to true after finished training model\nLOAD_MODEL = True\n\n# TRAIN_IMG_DIR = \"Data/train_images/\"\n# TRAIN_MASK_DIR = \"Data/train_masks/\"\n# VAL_IMG_DIR = \"Data/valid_images/\"\n# VAL_MASK_DIR = \"Data/valid_masks/\"\n\nROOT = os.path.join(os.getcwd(), \"MachineLearning\", \"Unet\", \"Data\")\nROOT2 = os.path.join(os.getcwd(), \"MachineLearning\", \"Unet\")\n\nTRAIN_IMG_DIR = os.path.join(ROOT, \"train_images\")\nTRAIN_MASK_DIR = os.path.join(ROOT, \"train_masks\")\nVAL_IMG_DIR = os.path.join(ROOT, \"valid_images\")\nVAL_MASK_DIR = os.path.join(ROOT, \"valid_masks\")","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.258591Z","iopub.execute_input":"2023-06-29T16:26:16.259022Z","iopub.status.idle":"2023-06-29T16:26:16.274391Z","shell.execute_reply.started":"2023-06-29T16:26:16.258962Z","shell.execute_reply":"2023-06-29T16:26:16.273431Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Train Function\nThe train function for one epoch","metadata":{}},{"cell_type":"code","source":"def train_fn(loader, model, optimizer, loss_fn, scaler):\n    # For progress bar\n    loop = tqdm(loader)\n    \n    # Iterates over each batch\n    for batch_idx, (data, targets) in enumerate(loop):\n#         display('data:')\n#         display(data.shape)\n#         display(data)\n        \n#         display('targets:')\n#         display(targets.shape)\n#         display(targets)\n        \n        data = data.to(device=DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n        \n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n        \n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        # update tqdm loop (showing loss function thus far)\n        loop.set_postfix(loss=loss.item())\n        \n        # ^^--- trains one epoch ---","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.277222Z","iopub.execute_input":"2023-06-29T16:26:16.277634Z","iopub.status.idle":"2023-06-29T16:26:16.292377Z","shell.execute_reply.started":"2023-06-29T16:26:16.277581Z","shell.execute_reply":"2023-06-29T16:26:16.290848Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"# MAIN FUNCTION:\ntrain_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0\n        ),\n        ToTensorV2()\n    ]\n)\n\nclass Paths:\n    data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming'\n    test_data_root = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/test'\n    contrails = '/kaggle/input/contrails-images-ash-color/contrails/'\n    train_path = '/kaggle/input/contrails-images-ash-color/train_df.csv'\n    valid_path = '/kaggle/input/contrails-images-ash-color/valid_df.csv'\n\n# Get dataframe\ntrain_df = pd.read_csv(Paths.train_path)\nvalid_df = pd.read_csv(Paths.valid_path)\n\ntrain_df['path'] = Paths.contrails + train_df['record_id'].astype(str) + '.npy'\nvalid_df['path'] = Paths.contrails + valid_df['record_id'].astype(str) + '.npy'\n\n# For multi-class segmentation, change \"out_channels\" to however many classes, and change BCEWithLogitsLoss to cross entropy loss\nmodel = UNET(in_channels=3, out_channels=1).to(DEVICE)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Gets the pytorch.DataLoader objects (i.e. the datasets except they're iterable)\ntrain_loader, val_loader = get_loaders(\n    train_df,\n    valid_df,\n    BATCH_SIZE,\n    NUM_WORKERS,\n    PIN_MEMORY\n)\n\n# if LOAD_MODEL:\n#     load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n\n#     # Resaves predictions in folder\n#     save_predictions_as_imgs(\n#         val_loader, model, folder=os.path.join(ROOT2, \"saved_images\"), device=DEVICE\n#     )\n\n#     # Do this after load model\n#     check_accuracy(val_loader, model, device=DEVICE)\n\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(NUM_EPOCHS):\n\n    print(f\"Training (epoch {epoch})\")\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    print(\"Saving model\")\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\":optimizer.state_dict()\n    }\n    save_checkpoint(checkpoint)\n\n    print(\"Check accuracy\")\n    # check accuracy\n    check_accuracy(val_loader, model, device=DEVICE)\n\n    # print some examples to a folder\n    # save_predictions_as_imgs(\n    #     val_loader, model, folder=\"saved_images/\", device=DEVICE\n    # )\n\n    print(\"Saving predictions as image\")\n\n    # Note: Only predicted images for validation images are saved. That's why they're so few.\n#     save_predictions_as_imgs(\n#         val_loader, model, folder=os.path.join(ROOT2, \"saved_images\"), device=DEVICE\n#     )","metadata":{"execution":{"iopub.status.busy":"2023-06-29T16:26:16.293533Z","iopub.execute_input":"2023-06-29T16:26:16.294340Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n","output_type":"stream"},{"name":"stdout","text":"Training (epoch 0)\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/1284 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n  0%|          | 2/1284 [01:47<18:55:58, 53.17s/it, loss=0.717]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Class for test dataset","metadata":{}},{"cell_type":"code","source":"class ContrailsTestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, train=True):\n        \n        self.df = df\n        self.trn = train\n    \n    # Handles reading from a directory under test (i.e. reads band files)\n    # Note: Only bands 11, 14, and 15 are used to create the final tensor\n    def read_record(self, directory):\n        \n        # Stores numpy arrays in a dictionary\n        record_data = {}\n        for x in [\n            \"band_11\", \n            \"band_14\", \n            \"band_15\"\n        ]:\n\n            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n\n        # Returns dictionary mapping band name (i.e. \"band_11\") to numpy array for that band file\n        # (i.e. band_11.npy converted to numpy array)\n        return record_data\n\n    def normalize_range(self, data, bounds):\n        \"\"\"Maps data to the range [0, 1].\"\"\"\n        return (data - bounds[0]) / (bounds[1] - bounds[0])\n    \n    # This is the function responsible for taking multiple bands as input, and returning a single tensor\n    # \"False color\" = \": color in an image (such as a photograph) of an object that does not actually \n    # appear in the object but is used to enhance, contrast, or distinguish details.\"\n    def get_false_color(self, record_data):\n        _T11_BOUNDS = (243, 303)\n        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n        _TDIFF_BOUNDS = (-4, 2)\n        \n        N_TIMES_BEFORE = 4\n        \n        # \"Combines\" the 'band_15', 'band_14', and 'band_11' tensors (each are 3D, H x W x Time step)\n        # Note: r, g, and b are still H x W x Time step (256 x 256 x 8)\n        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n        \n        display('rgb:')\n        display(len(r))\n        display(len(r[0]))\n        display(len(r[0][0]))\n        display(g)\n        display(b)\n        \n        # Combines 3 x (H x W x T or 256 x 256 x 8) tensors into one H x W x C x T (256 x 256 x 3 x 8)\n        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n        display('false_color')\n        display(false_color.shape)\n        \n        # Slice includes all dimensions 1 - 3, but only the 'N_TIMES_BEFORE'th 4th dimension element\n        img = false_color[..., N_TIMES_BEFORE]\n\n        return img\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        con_path = row.path\n        data = self.read_record(con_path)    \n        \n        display('getting data')\n#         display(data)\n        display('band_11')\n        display(len(data['band_11']))\n        display(len(data['band_11'][0]))\n        display(len(data['band_11'][0][0]))\n        img = self.get_false_color(data)\n        \n        display('img:')\n        display(len(img))\n        display(len(img[0]))\n        display(len(img[0][0]))\n        display(img)\n        \n        img = torch.tensor(img)\n        \n        display('converted to tensor')\n        display(img)\n        \n        # Changes tensor shape from H x W x C (256 x 256 x 3) to C x H x W (3 x 256 x 256)\n        img = img.permute(2, 0, 1)\n        \n        display('ret img')\n        display(img.shape)\n        \n        return img.float()\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get dataset","metadata":{}},{"cell_type":"code","source":"filenames = os.listdir(Paths.test_data_root)\ntest_df = pd.DataFrame(filenames, columns=['record_id'])\n\ntest_df['path'] = Paths.test_data_root + test_df['record_id'].astype(str)\n\n\ntest_ds = ContrailsTestDataset(\n        test_df,\n        train = False\n    )\n\ndisplay(len(test_ds))\ndisplay(test_ds)\ndisplay(len(test_ds[0]))\ndisplay(test_ds[0])\n\ntest_dl = DataLoader(test_ds, batch_size=Config.batch_size, num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nGenerates predictions for test data here","metadata":{}},{"cell_type":"code","source":"test_preds = []\n\nprogress_bar = tqdm(test_dl)\n\nfor i, x in enumerate(progress_bar)\n    with torch.cuda.amp.autocast():\n        pred = model(data)\n        test_preds.append(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert to RLE format\nCourtesy of Inversion :)","metadata":{}},{"cell_type":"code","source":"def rle_encode(x, fg_val=1):\n    \"\"\"\n    Args:\n        x:  numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns: run length encoding as list\n    \"\"\"\n\n    dots = np.where(\n        x.T.flatten() == fg_val)[0]  # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\n\ndef list_to_string(x):\n    \"\"\"\n    Converts list to a string representation\n    Empty list returns '-'\n    \"\"\"\n    if x: # non-empty list\n        s = str(x).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n    else:\n        s = '-'\n    return s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_preds = [list_to_string(rle_encode(i)) for i in test_preds]\n\ntest_ids = os.listdir(Paths.test_dataset_root)\n\nsubmission = pd.read_csv(pd.DataFrame('record_id':test_ids, 'encoded_pixels':test_preds))\n\nsubmission.head()\n\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}